{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c300615b",
   "metadata": {},
   "source": [
    "# Politecnico Malvinas Argentinas- PDH: Preprocesamiento, Normalización y Técnicas Básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e327cbc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sil46\\anaconda3\\lib\\site-packages (3.8.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\sil46\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\sil46\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\sil46\\anaconda3\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: spacy in c:\\users\\sil46\\anaconda3\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sil46\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: thefuzz in c:\\users\\sil46\\anaconda3\\lib\\site-packages (0.22.1)\n",
      "Requirement already satisfied: stop_words in c:\\users\\sil46\\anaconda3\\lib\\site-packages (2018.7.23)\n",
      "Requirement already satisfied: click in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (70.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from thefuzz) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\sil46\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.27.1-cp311-cp311-win_amd64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in c:\\users\\sil46\\anaconda3\\lib\\site-packages (from Levenshtein==0.27.1->python-Levenshtein) (3.12.2)\n",
      "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading levenshtein-0.27.1-cp311-cp311-win_amd64.whl (100 kB)\n",
      "Installing collected packages: Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.27.1 python-Levenshtein-0.27.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\sil46\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\sil46\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\sil46\\anaconda3\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerías necesarias (si es necesario en Colab)\n",
    "!pip install nltk spacy scikit-learn thefuzz stop_words\n",
    "!pip install fuzzywuzzy python-Levenshtein\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69c6980a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sil46\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sil46\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d7e5601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.8.0/es_core_news_md-3.8.0-py3-none-any.whl (42.3 MB)\n",
      "     ---------------------------------------- 0.0/42.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/42.3 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.8/42.3 MB 3.4 MB/s eta 0:00:13\n",
      "     - -------------------------------------- 1.3/42.3 MB 3.9 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 2.4/42.3 MB 3.7 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 3.1/42.3 MB 3.8 MB/s eta 0:00:11\n",
      "     --- ------------------------------------ 4.2/42.3 MB 4.0 MB/s eta 0:00:10\n",
      "     ----- ---------------------------------- 5.8/42.3 MB 4.6 MB/s eta 0:00:08\n",
      "     ------ --------------------------------- 7.1/42.3 MB 4.8 MB/s eta 0:00:08\n",
      "     ------- -------------------------------- 8.1/42.3 MB 4.9 MB/s eta 0:00:07\n",
      "     -------- ------------------------------- 9.4/42.3 MB 5.1 MB/s eta 0:00:07\n",
      "     ---------- ----------------------------- 11.0/42.3 MB 5.3 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 12.3/42.3 MB 5.4 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 13.6/42.3 MB 5.5 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 14.9/42.3 MB 5.5 MB/s eta 0:00:05\n",
      "     -------------- ------------------------- 15.7/42.3 MB 5.5 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 16.3/42.3 MB 5.3 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 16.5/42.3 MB 5.1 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 17.6/42.3 MB 4.9 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 18.6/42.3 MB 4.9 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 19.4/42.3 MB 4.9 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 20.7/42.3 MB 4.9 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 21.8/42.3 MB 4.9 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 22.3/42.3 MB 4.8 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 22.8/42.3 MB 4.7 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 23.3/42.3 MB 4.6 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 23.9/42.3 MB 4.6 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 24.4/42.3 MB 4.5 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 24.4/42.3 MB 4.5 MB/s eta 0:00:05\n",
      "     ------------------------ --------------- 25.4/42.3 MB 4.3 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 26.0/42.3 MB 4.3 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 26.5/42.3 MB 4.2 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 27.0/42.3 MB 4.1 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 27.3/42.3 MB 4.1 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 27.8/42.3 MB 4.1 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 28.3/42.3 MB 4.0 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 28.8/42.3 MB 4.0 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 29.4/42.3 MB 3.9 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 30.1/42.3 MB 3.9 MB/s eta 0:00:04\n",
      "     ---------------------------- ----------- 30.4/42.3 MB 3.9 MB/s eta 0:00:04\n",
      "     ----------------------------- ---------- 30.9/42.3 MB 3.8 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 31.5/42.3 MB 3.8 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 31.5/42.3 MB 3.8 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 32.5/42.3 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 33.3/42.3 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 33.6/42.3 MB 3.7 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 34.1/42.3 MB 3.6 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 34.9/42.3 MB 3.6 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 35.1/42.3 MB 3.6 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 35.7/42.3 MB 3.6 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.2/42.3 MB 3.5 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.4/42.3 MB 3.5 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.7/42.3 MB 3.4 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 36.7/42.3 MB 3.4 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 37.2/42.3 MB 3.4 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 37.7/42.3 MB 3.4 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 38.3/42.3 MB 3.3 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 38.8/42.3 MB 3.3 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 39.3/42.3 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 39.8/42.3 MB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 40.4/42.3 MB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 40.9/42.3 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  41.4/42.3 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  41.9/42.3 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  42.2/42.3 MB 3.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 42.3/42.3 MB 3.2 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\sil46\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\sil46\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\sil46\\anaconda3\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo en español de spaCy\n",
    "!python -m spacy download es_core_news_md\n",
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4276252",
   "metadata": {},
   "source": [
    "# 1. Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c111ae",
   "metadata": {},
   "source": [
    "# Tokenizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8203c02",
   "metadata": {},
   "source": [
    "Quizás una de las operaciones más básicas que podemos hacer es contar palabras. Imaginemos un caso extremadamente sencillo relacionado al aprendizaje automático. Pensemos en un clasificador de positividad / negatividad.\n",
    "\n",
    "Podríamos usar una regla como:\n",
    "\n",
    "si \"bueno\" está en el texto, clasificamos como positivo.\n",
    "\n",
    "Claramente un enfoque tan sencillo va a ser propenso a muchos errores. Iremos viendo cómo aplicarlo y mejorarlo.\n",
    "\n",
    "La operación de separar los textos en unidades básicas (o tokens) la llamamos tokenización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb561ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Usualmente,',\n",
       " 'existe',\n",
       " 'una',\n",
       " 'relación',\n",
       " 'costo-beneficio',\n",
       " 'entre',\n",
       " 'las',\n",
       " 'distintas',\n",
       " 'técnicas.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = 'Usualmente, existe una relación costo-beneficio entre las distintas técnicas.'\n",
    "doc2.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313443a7",
   "metadata": {},
   "source": [
    "Vemos que en castellano, el lenguaje natural (escrito) puede tener distintas fuentes de variaciones:\n",
    "\n",
    "Mayúsculas\n",
    "Tildes\n",
    "Signos ortográficos\n",
    "Errores de tipeo\n",
    "Variaciones propias del lenguaje como la conjugación\n",
    "Artefactos de la escritura informal como \"holaaa\"\n",
    "¿Cómo abordarán esto nuestros algorítmos?\n",
    "\n",
    "Una solución es la normalización: con acepción similar aunque distinta a la que usamos en estadística, va a significar transformar a los strings a representaciones iguales.\n",
    "\n",
    "Podemos:\n",
    "\n",
    "Sacar espacios\n",
    "Pasar a minúsculas\n",
    "Quitar tildes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a5778e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e4895fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usualmente,',\n",
       " 'existe',\n",
       " 'una',\n",
       " 'relacion',\n",
       " 'costo-beneficio',\n",
       " 'entre',\n",
       " 'las',\n",
       " 'distintas',\n",
       " 'tecnicas.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unidecode\n",
    "[unidecode.unidecode(w.lower().strip()) for w in doc2.split(' ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a497000",
   "metadata": {},
   "source": [
    "Otras normalizaciones más sofisticadas serían:\n",
    "- Pasar de plural a singular\n",
    "- Convertir el género de la palabra\n",
    "- Conjugar los verbos en infinitivo\n",
    "\n",
    "¡Es de esperarse que esto requiera métodos más complejos!\n",
    "\n",
    "Vemos que con la tokenización anterior, todavía tenemos palabras con signos de puntuación pegados. Armar nuestro vocabulario es un paso clave para nuestros modelos de *Machine Learning*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40433b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenización y Normalización: ['¡hola', '!', '¿cómo', 'estás', '?', 'estoy', 'aprendiendo', 'nlp', 'con', 'spacy', 'y', 'nltk', '.', 'nlp', 'es', 'fascinante', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"¡Hola! ¿Cómo estás? Estoy aprendiendo NLP con spaCy y NLTK. NLP es fascinante.\"\n",
    "\n",
    "# Tokenización y Normalización\n",
    "words = word_tokenize(text.lower())\n",
    "print(\"Tokenización y Normalización:\", words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09f28efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto limpio con regex: hola cómo estás estoy aprendiendo nlp con spacy y nltk nlp es fascinante\n"
     ]
    }
   ],
   "source": [
    "# Expresiones Regulares\n",
    "text_clean = re.sub(r'[^a-zA-Záéíóúüñ ]', '', text.lower())\n",
    "print(\"Texto limpio con regex:\", text_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57824cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin Stop Words: ['¡hola', '!', '¿cómo', '?', 'aprendiendo', 'nlp', 'spacy', 'nltk', '.', 'nlp', 'fascinante', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "print(\"Sin Stop Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b576e0f",
   "metadata": {},
   "source": [
    "# 2. Técnicas de NLP con NLTK y spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a10f35b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: ['¡hol', '!', '¿com', '?', 'aprend', 'nlp', 'spacy', 'nltk', '.', 'nlp', 'fascin', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming con NLTK\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "print(\"Stemming:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fec19e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatización: ['¡', 'hola', '!', '¿', 'cómo', '?', 'aprender', 'nlp', 'spacy', 'nltk', '.', 'nlp', 'fascinante', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatización con spaCy\n",
    "doc = nlp(\" \".join(filtered_words))\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(\"Lemmatización:\", lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45b660f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distancia fuzzy entre 'casa' y 'casas': 89\n"
     ]
    }
   ],
   "source": [
    "# Distancias entre strings\n",
    "print(\"Distancia fuzzy entre 'casa' y 'casas':\", fuzz.ratio(\"casa\", \"casas\"))\n",
    "texts = [\"El gato come pescado\", \"El perro ladra en la casa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16823d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\sil46\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\sil46\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\sil46\\anaconda3\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyjarowinkler\n",
      "  Downloading pyjarowinkler-2.0.0-py3-none-any.whl.metadata (20 kB)\n",
      "Downloading pyjarowinkler-2.0.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pyjarowinkler\n",
      "Successfully installed pyjarowinkler-2.0.0\n"
     ]
    }
   ],
   "source": [
    "# segundo ejemplo %%capture\n",
    "!pip install pyjarowinkler\n",
    "from nltk.metrics import edit_distance\n",
    "from pyjarowinkler import distance as jwdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ffd7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pero' vs 'perro':\n",
      "Distancia Levenshtein -> 1\n",
      "Similitud Jaro Winkler -> 0.07\n",
      "----------------------------------------\n",
      "'pero' vs 'pierdo':\n",
      "Distancia Levenshtein -> 2\n",
      "Similitud Jaro Winkler -> 0.11\n",
      "----------------------------------------\n",
      "'nueve' vs 'mueve':\n",
      "Distancia Levenshtein -> 1\n",
      "Similitud Jaro Winkler -> 0.13\n",
      "----------------------------------------\n",
      "'totalmente' vs 'diferentes':\n",
      "Distancia Levenshtein -> 7\n",
      "Similitud Jaro Winkler -> 0.48\n",
      "----------------------------------------\n",
      "'pero' vs 'basta':\n",
      "Distancia Levenshtein -> 5\n",
      "Similitud Jaro Winkler -> 1.0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Definimos una lista de tuplas de palabras\n",
    "palabras = [(\"pero\", \"perro\"),(\"pero\", \"pierdo\"), (\"nueve\", \"mueve\"),  (\"totalmente\",\"diferentes\"), (\"pero\", \"basta\")]\n",
    "\n",
    "# Calculamos las metricas de distancia pasando cada tupla como argumentos a levdist() y get_jaro_distance()\n",
    "for x,y in palabras:\n",
    "    print(f\"'{x}' vs '{y}':\")\n",
    "    print(\"Distancia Levenshtein ->\", edit_distance(x,y))\n",
    "    print(\"Similitud Jaro Winkler ->\",jwdist.get_jaro_distance(x,y))\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644ccfb7",
   "metadata": {},
   "source": [
    "# 3. Vectorización y Representación del Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74ffa495",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"El gato come pescado\", \"El perro ladra en la casa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "988df847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>casa</th>\n",
       "      <th>come</th>\n",
       "      <th>el</th>\n",
       "      <th>en</th>\n",
       "      <th>gato</th>\n",
       "      <th>la</th>\n",
       "      <th>ladra</th>\n",
       "      <th>perro</th>\n",
       "      <th>pescado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   casa  come  el  en  gato  la  ladra  perro  pescado\n",
       "0     0     1   1   0     1   0      0      0        1\n",
       "1     1     0   1   1     0   1      1      1        0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bolsa de Palabras (BoW)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "55277aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>casa</th>\n",
       "      <th>come</th>\n",
       "      <th>el</th>\n",
       "      <th>en</th>\n",
       "      <th>gato</th>\n",
       "      <th>la</th>\n",
       "      <th>ladra</th>\n",
       "      <th>perro</th>\n",
       "      <th>pescado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.379978</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.534046</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.534046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.303216</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.42616</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      casa      come        el       en      gato       la    ladra    perro  \\\n",
       "0  0.00000  0.534046  0.379978  0.00000  0.534046  0.00000  0.00000  0.00000   \n",
       "1  0.42616  0.000000  0.303216  0.42616  0.000000  0.42616  0.42616  0.42616   \n",
       "\n",
       "    pescado  \n",
       "0  0.534046  \n",
       "1  0.000000  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(texts)\n",
    "pd.DataFrame(X_tfidf.toarray(), columns=vectorizer_tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e382141a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>come pescado</th>\n",
       "      <th>el gato</th>\n",
       "      <th>el gato come</th>\n",
       "      <th>el perro</th>\n",
       "      <th>el perro ladra</th>\n",
       "      <th>en la</th>\n",
       "      <th>en la casa</th>\n",
       "      <th>gato come</th>\n",
       "      <th>gato come pescado</th>\n",
       "      <th>la casa</th>\n",
       "      <th>ladra en</th>\n",
       "      <th>ladra en la</th>\n",
       "      <th>perro ladra</th>\n",
       "      <th>perro ladra en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   come pescado  el gato  el gato come  el perro  el perro ladra  en la  \\\n",
       "0             1        1             1         0               0      0   \n",
       "1             0        0             0         1               1      1   \n",
       "\n",
       "   en la casa  gato come  gato come pescado  la casa  ladra en  ladra en la  \\\n",
       "0           0          1                  1        0         0            0   \n",
       "1           1          0                  0        1         1            1   \n",
       "\n",
       "   perro ladra  perro ladra en  \n",
       "0            0               0  \n",
       "1            1               1  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-gramas\n",
    "vectorizer_ngram = CountVectorizer(ngram_range=(2,3), min_df=1)\n",
    "X_ngram = vectorizer_ngram.fit_transform(texts)\n",
    "pd.DataFrame(X_ngram.toarray(), columns=vectorizer_ngram.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a215361e",
   "metadata": {},
   "source": [
    "# 4. Tareas Avanzadas con spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21aeeb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"El presidente de Argentina visitó Buenos Aires.\"\n",
    "doc = nlp(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce6e9ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('El', 'DET'), ('presidente', 'NOUN'), ('de', 'ADP'), ('Argentina', 'PROPN'), ('visitó', 'VERB'), ('Buenos', 'PROPN'), ('Aires', 'PROPN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "print([(token.text, token.pos_) for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5180d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Argentina', 'LOC'), ('Buenos Aires', 'LOC')]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "112d3952",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'displacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Dependencias sintácticas\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m displacy\u001b[38;5;241m.\u001b[39mrender(doc, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdep\u001b[39m\u001b[38;5;124m'\u001b[39m, jupyter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'displacy' is not defined"
     ]
    }
   ],
   "source": [
    "# Dependencias sintácticas\n",
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7551abbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW no captura el significado de las palabras ni el orden de las mismas.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Limitaciones de Bolsa de Palabras\n",
    "print(\"BoW no captura el significado de las palabras ni el orden de las mismas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a44c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
